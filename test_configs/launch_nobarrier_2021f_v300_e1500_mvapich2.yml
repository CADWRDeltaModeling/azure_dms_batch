resource_group: dwrbdo_schism_rg # the resource group containing the batch account
job_name: mss_nobarrier_2021f_v300_e1500 # job name, will be used to name the pool and the job
batch_account_name: schismbatch # batch account name
#start_task_script: "printenv && $AZ_BATCH_APP_PACKAGE_batch_setup_alma8_7/batch/pool_setup.sh"
storage_account_name: dwrbdoschismsa # this is the storage account containing batch and storage_container defined below
storage_container_name: mss2024 # this is mounted to $AZ_BATCH_MOUNTS_DIR/<<storage_container_name>> in addition to batch container which is mounted to $AZ_BATCH_MOUNTS_DIR/batch
#study_copy_flags: --recursive --preserve-symlinks
study_dir: simulations/mss_nobarrier_2021f_v300_e1500 # e.g. for test container this is vansickle2020/base_barclinic # this is copied sans the outputs/ folder 
setup_dirs: # these are directories that are also copied in addition to the study_dir
 - hrrr # e.g. for test container this is vansickle2020/ts
# - vansickle2020/atmospheric_data # e.g. for test container this is vansickle2020/inputs
vm_size: "Standard_HB120rs_v2" # Standard_HB120rs_v3
num_hosts: 2 # number of nodes in the pool
# num_cores: <<number of cores total>> # is optional as default is number of cores per host * number of hosts
num_scribes: 7 # This is used in the mpi_cmd template if referred to there
# command to run , assume the study_dir is current directory
# mpi_command: "printenv"
# UCX_TLS=dc,sm
# --bind-to core --map-by ppr:3:numa
mpi_opts: --bind-to core
mpi_command: |
 cd sflux; rm -f *.nc;  python3 make_links_full.py; cd ../; 
 mpirun -np {num_cores} -f hostfile {mpi_opts} pschism_PREC_EVAP_GOTM_GEN_TVD-VL {num_scribes}
template_name: "alma87_mvapich2_20240426" # this is the template name for the pool, e.g. "centos7" or "alma8"